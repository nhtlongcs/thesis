\subsection{Relation Modeling}
\label{sec:relation_modeling}
In this section, we focus on extracting relationships between the target and those nearby vehicles. 
First we custom the DeepSort algorithm to efficiently track all vehicles attending on each video, resulting in a list of trajectories beside the provided one. Then we use these tracklets to gather all location, visual cues and perform classification prediction tasks (discussed in section ...) to achieve a detailed description of each existing vehicle including appearance attributes (vehicle type, color) and action types.
From this information, we then build a module to analyse the movement direction, relative distances between the target vehicle and all detected vehicles and determine if the target \textit{is following} them or \textit{being followed} by the others. 
Figure ... illustrates the workflow of our proposed process.

\subsubsection{Exploring surrounding objects}
Given a video track, we adapt a pre-trained object detector to localize all appearing vehicles on each frame. Then we adapt a feature extractor trained with ReID settings to obtain the appearance feature of each bounding box. The tracking algorithm then greedily compares similarity between these boxes frame-by-frame to associate all potential vehicles. Two consecutive boxes are seen as indicating the same object when their appearance similarity is lower than a pre-defined threshold. 

The AI City Challenge is launched with the intention of solving real-world problems, hence the provided training dataset contains lots of noises, which usually appear in practical scenarios. As a result, we encountered following problems when performing tracking:
\begin{itemize}
    \item High miss-frame video. In such cases, frames do not align with the correct time step. Figure … plots the distribution of miss frames in training set and test set. In some cases with extremely high miss rate (over 400 frames), the vehicles could suddenly change their positions or disappear from the video. This noise directly damages the tracking performance, especially when applying IoU distance for similarity measurement.
    \item Uncertain vehicles. From the view of traffic cameras, there are some areas where the vehicles are occluded by other obstacles (trees, traffic signs, etc.) or when the vehicles move far away from the cameras’ receptive field, they become noises for the detector.
\end{itemize}
To overcome the missed detection issue, we first generate an attention area for each video track to force the tracking algorithm to focus on those high confident detections only during the matching process. 
The attention masks are generated as follow:
\begin{itemize}
    \item Given the target vehicle tracklet, we produce a smooth trajectory by generating boxes for those missed frames with the assumption that the vehicle move linearly in the uncaptured time. For two bounding boxes $B_k$, $B_{k+1}$ corresponding to frames $F_k$ and $F_{k+1}$, the bounding boxes in between are interpolated by
    \begin{align}
        B_c = \frac{F_{k+1} - F_c}{F_{k+1} - F_k} \times B_k + \frac{F_{c} - F_k}{F_{k+1} - F_k} \times B_{k + 1}, \quad F_c \in (F_k, F_{k+1})
    \end{align}
    where $B_c$ denotes the bounding box at frame $F_c$. 
    \item Given the smooth trajectory, we enlarge the bounding box at each time step by a scale ratio $\alpha$ to create an attention mask. The final attention area of each video is the union of those masks through time.
\end{itemize}
Then, we remove those boxes that lie out of the attention area, which usually contains small vehicles, missed detections or too far vehicles. Figure ... shows a result of this step.

\subsubsection{Tracking-based relation modeling}

